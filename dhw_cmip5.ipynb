{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ph290/miniconda2/lib/python2.7/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: The mpl_toolkits.axes_grid module was deprecated in version 2.1. Use mpl_toolkits.axes_grid1 and mpl_toolkits.axisartist provies the same functionality instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import iris.quickplot as qplt\n",
    "import netCDF4\n",
    "import datetime\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import glob\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "directory = '/Users/ph290/Downloads/revelle2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_region(cube,lon_west,lon_east,lat_south,lat_north):\n",
    "    cube_region_tmp = cube.intersection(longitude=(lon_west, lon_east))\n",
    "    cube_region = cube_region_tmp.intersection(latitude=(lat_south, lat_north))\n",
    "    return cube_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_avg(cube):\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    return cube.collapsed(['longitude','latitude'],iris.analysis.MEAN, weights=grid_areas)\n",
    "\n",
    "def area_max(cube):\n",
    "    return cube.collapsed(['longitude','latitude'],iris.analysis.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmm_for_dhm(cube):\n",
    "    years_for_mmm_climatology = [1985,2000]\n",
    "    #####################################################\n",
    "    #Avreage months separately!!!!!!!!!!!\n",
    "#####################################################\n",
    "    cube_years = cube.coord('year').points\n",
    "    #subset the data into the bit you want to use to calculate the MMM climatology and the bit you want to calculate DHW on\n",
    "    clim_cube = cube[np.where((cube_years >= years_for_mmm_climatology[0]) & (cube_years <= years_for_mmm_climatology[1]))]\n",
    "    #collapse the months together, taking the maximum value at each lat-lon grid square\n",
    "    mmm_climatology = clim_cube.collapsed('time',iris.analysis.MAX)\n",
    "    return mmm_climatology\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhm(cube,mmm_climatology,years_over_which_to_calculate_dhm):\n",
    "    missing_data_value = cube.data.fill_value\n",
    "    #This hsould be given monthly data\n",
    "    # One DHM == 4DHW This is important\n",
    "#     mmm is straight averg not fdetreneded for DHM\n",
    "    #Look at the donna poapers - look over two papers...\n",
    "    #accumulation window is 4 months rather than 3 months.\n",
    "    cube_years = cube.coord('year').points\n",
    "#     print cube_years\n",
    "#     print years_over_which_to_calculate_dhm[0]\n",
    "#     print years_over_which_to_calculate_dhm[1]\n",
    "    main_cube = cube[np.where((cube_years >= years_over_which_to_calculate_dhm[0]) & (cube_years <= years_over_which_to_calculate_dhm[1]))]\n",
    "    main_cube_data = main_cube.data.copy()\n",
    "    main_cube_data[np.where(main_cube_data == missing_data_value )] = np.nan\n",
    "    main_cube.data = main_cube_data\n",
    "    \n",
    "    #subtract the monthly mean climatology from the rest of the data\n",
    "    main_cube -= mmm_climatology # at this stage this is called a hot spot (which is anything greater than the mmm)\n",
    "\n",
    "    #set all values less than 1 to zero\n",
    "#     main_cube.data[np.where(main_cube.data <= 1.0)] = 0.0\n",
    "    #OR\n",
    "    main_cube_data = main_cube.data.copy()\n",
    "    main_cube_data[np.where(main_cube_data < 0.0)] = 0.0\n",
    "#     main_cube_data[np.where(main_cube_data == missing_data_value )] = np.nan\n",
    "    main_cube.data = main_cube_data\n",
    "    \n",
    "    #make a cube to hold the output data\n",
    "    output_cube = main_cube[3::].copy()\n",
    "    output_cube.data[:] = np.nan\n",
    "    output_cube_data = output_cube.data.copy()\n",
    "\n",
    "        #AVEREG OVER A 4 month  window rather than 3 month when it comes to DHW\n",
    "\n",
    "\n",
    "    #loop through from day 112 to the end of the dataset\n",
    "    for i in range(output_cube.shape[0]):\n",
    "#         print i,' of ',output_cube.shape[0]\n",
    "        tmp_data = main_cube[i:i+4].collapsed('time',iris.analysis.SUM)\n",
    "        output_cube_data[i,:,:] = tmp_data.data\n",
    "\n",
    "    #save the output\n",
    "    output_cube.data = output_cube_data\n",
    "    return output_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_cubes(cubes):\n",
    "    data1 = cubes[0].data\n",
    "    data2 = cubes[1].data\n",
    "    data = np.concatenate([data1,data2],axis=0)\n",
    "    data = np.ma.masked_where(data == cubes[0].data.fill_value, data)\n",
    "\n",
    "    length = cubes[0].shape[0] + cubes[1].shape[0]\n",
    "    datetime_object1 = netCDF4.num2date(cubes[0].coord('time').points,cubes[0].coord('time').units.name,cubes[0].coord('time').units.calendar)\n",
    "    datetime_object2 = netCDF4.num2date(cubes[1].coord('time').points,cubes[1].coord('time').units.name,cubes[1].coord('time').units.calendar)\n",
    "    datetime_object = np.concatenate([datetime_object1,datetime_object2])\n",
    "    try:\n",
    "        tmp =  [x._to_real_datetime() - datetime.datetime(1850,1,1) for x in datetime_object]\n",
    "    except:\n",
    "        tmp =  [x - datetime.datetime(1850,1,1) for x in datetime_object]\n",
    "    days_since_18500101 = [x.days for x in tmp]\n",
    "\n",
    "    time = iris.coords.DimCoord(days_since_18500101, standard_name='time',long_name=u'time', var_name='time', units='days since 1850-1-1')\n",
    "    latitude = iris.coords.DimCoord(range(-90, 90, 1), standard_name='latitude', units='degrees')\n",
    "    longitude = iris.coords.DimCoord(range(0, 360, 1), standard_name='longitude', units='degrees')\n",
    "    cube = iris.cube.Cube(data,standard_name='sea_surface_temperature',long_name='Sea Surface Temperature', var_name='tos', units='K',dim_coords_and_dims=[(time,0), (latitude, 1),\n",
    "    (longitude, 2)])\n",
    "    iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    iris.coord_categorisation.add_month(cube, 'time', name='month')\n",
    "    iris.coord_categorisation.add_month_number(cube, 'time', name='month_number')\n",
    "    return cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the dictonary with model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['BCC-CSM2-MR','CanESM5','MPI-ESM1-2-HR','CESM2','MIROC6','MRI-ESM2-0']\n",
    "\n",
    "colors = ['r','g','b','k','m','c']\n",
    "model_dict = {}\n",
    "for i,model in enumerate(models):\n",
    "    model_dict[model] = {}\n",
    "    model_dict[model]['color'] = colors[i]\n",
    "\n",
    "\n",
    "# BCC-CSM2-MR_tos_historical_r1i1p1f1_gn.nc\tBCC-CSM2-MR_tos_ssp585_r1i1p1f1_gn.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in the model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ph290/miniconda2/lib/python2.7/site-packages/iris/fileformats/cf.py:798: UserWarning: Missing CF-netCDF measure variable u'areacello', referenced by netCDF variable u'tos'\n",
      "  warnings.warn(message % (variable_name, nc_var_name))\n",
      "/Users/ph290/miniconda2/lib/python2.7/site-packages/iris/fileformats/cf.py:1143: IrisDeprecation: NetCDF default loading behaviour currently does not expose variables which define reference surfaces for dimensionless vertical coordinates as independent Cubes. This behaviour is deprecated in favour of automatic promotion to Cubes. To switch to the new behaviour, set iris.FUTURE.netcdf_promote to True.\n",
      "  warn_deprecated(msg)\n"
     ]
    }
   ],
   "source": [
    "cube_dict = {}\n",
    "\n",
    "for model in models:\n",
    "    cube_dict[model] = {}\n",
    "    # file = glob.glob(model+'_tos_*_r1i1p1f1_gn.nc')\n",
    "    # print file\n",
    "\n",
    "    cubes = iris.load(directory+model+'_tos_*_r1i1p1f1_gn.nc')\n",
    "    # cubes2 = iris.load(model+'_tos_ssp585_r1i1p1f1_gn.nc')\n",
    "    # cubes = iris.cube.CubeList([cubes1,cubes2])\n",
    "\n",
    "    cube =  merge_two_cubes(cubes)\n",
    "    cube_dict[model]['cube'] = cube\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IGNORE FOR NOW testing with 3hourly sst to compare day and night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = '/data/BatCaveNAS/ph290/cmip6/tmp2/processed/tos_3hr_MPI-ESM1-2-HR_historical_r1i1p1f1_gn_regridded.nc'\n",
    "# cube = iris.load_cube(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate DHMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCC-CSM2-MR\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'years_for_mmm_climatology' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a9c1767f8bbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0myears_over_which_to_calculate_dhm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2012\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmmm_climatology\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmm_for_dhm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcube_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cube'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myears_for_mmm_climatology\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdhm_cube\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdhm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcube_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cube'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmmm_climatology\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myears_over_which_to_calculate_dhm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlon_west\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m142.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'years_for_mmm_climatology' is not defined"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print model\n",
    "    years_over_which_to_calculate_dhm = [2012,2100]\n",
    "    mmm_climatology = mmm_for_dhm(cube_dict[model]['cube'])\n",
    "    dhm_cube = dhm(cube_dict[model]['cube'],mmm_climatology,years_over_which_to_calculate_dhm)\n",
    "    lon_west = 142.0\n",
    "    lon_east = 157.0\n",
    "    lat_south = -30.0\n",
    "    lat_north = -10.0\n",
    "    dhm_cube_gbr = extract_region(dhm_cube,lon_west,lon_east,lat_south,lat_north)\n",
    "    cube_dict[model]['dhm'] = dhm_cube_gbr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asb(cube,threshold):\n",
    "    dhm_cube_gbr_tmp = cube.copy()\n",
    "    dhm_cube_gbr_tmp_data = dhm_cube_gbr_tmp.data\n",
    "    dhm_cube_gbr_tmp_data[np.where(dhm_cube_gbr_tmp_data <= threshold)] = 0.0\n",
    "    dhm_cube_gbr_tmp_data[np.where(dhm_cube_gbr_tmp_data > threshold)] = 1.0\n",
    "    dhm_cube_gbr_tmp.data = dhm_cube_gbr_tmp_data\n",
    "    dhm_cube_gbr_asb = dhm_cube_gbr.copy()\n",
    "    dhm_cube_gbr_asb = dhm_cube_gbr_tmp.aggregated_by(['year'], iris.analysis.SUM)\n",
    "    dhm_cube_gbr_asb_tmp = dhm_cube_gbr_asb.data\n",
    "    dhm_cube_gbr_asb_tmp[np.where(dhm_cube_gbr_asb_tmp > 1.0)] = 1.0\n",
    "    dhm_cube_gbr_asb.data = dhm_cube_gbr_asb_tmp\n",
    "    return dhm_cube_gbr_asb\n",
    "    \n",
    "for model in models:\n",
    "    dhm_cube_gbr = extract_region(cube_dict[model]['dhm'],lon_west,lon_east,lat_south,lat_north)\n",
    "    dhm_cube_gbr_asb = asb(dhm_cube_gbr,2.0) # 2.0 because that is equivilantish to dhw of 4\n",
    "    cube_dict[model]['dhm_gbr'] = dhm_cube_gbr\n",
    "    cube_dict[model]['asb'] = dhm_cube_gbr_asb\n",
    "    cube_dict[model]['asb_avg'] = area_avg(dhm_cube_gbr_asb)\n",
    "    cube_dict[model]['dhm_avg'] = area_avg(dhm_cube_gbr.aggregated_by(['year'], iris.analysis.MEAN))\n",
    "    cube_dict[model]['asb_max'] = area_max(dhm_cube_gbr_asb)\n",
    "    cube_dict[model]['dhm_max'] = area_max(dhm_cube_gbr.aggregated_by(['year'], iris.analysis.MAX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual Severe Bleaching (defined as a DHM > 2.0) per model per year, area averaged. 1.0 on the y-axis would mean that every grid cell experienced ASB in that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "\n",
    "for model in models:\n",
    "#     print cube_dict[model]['asb'].coord('year').points\n",
    "#     y = cube_dict[model]['asb_avg'].data\n",
    "#     y[np.where(y > 0.0)] = 1\n",
    "    plt.scatter(cube_dict[model]['asb'].coord('year').points,cube_dict[model]['asb_avg'].data,alpha=0.5,color=model_dict[model]['color'],label = model)\n",
    "\n",
    "# plt.ylim([-0.1,1.1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# for model in models:\n",
    "#     qplt.plot(cube_dict[model]['dhm_cube_Spillman_2013_avg'],lw = 2,alpha=0.5,color=model_dict[model]['color'])\n",
    "\n",
    "    \n",
    "for model in models:\n",
    "    qplt.plot(cube_dict[model]['dhm_avg_avg'],lw=2,color=model_dict[model]['color'],label = model,alpha=0.5)\n",
    "\n",
    "\n",
    "# plt.ylim([0,0.5])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "land_110m = cfeature.NaturalEarthFeature('physical', 'land', '110m',edgecolor='face',facecolor=(0.85, 0.85, 0.85))\n",
    "\n",
    "for i,model in enumerate(models):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "    qplt.pcolormesh(cube_dict[model]['asb'][0:20].collapsed('time',iris.analysis.MAX),vmin=0,vmax=1)\n",
    "    ax.add_feature(land_110m)\n",
    "    plt.gca().coastlines()\n",
    "    plt.title(model)\n",
    "    qplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
