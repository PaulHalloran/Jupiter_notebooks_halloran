{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORK ON NATIVE GRIDS!!! This will allow access to more models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ph290/miniconda2/lib/python2.7/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: The mpl_toolkits.axes_grid module was deprecated in version 2.1. Use mpl_toolkits.axes_grid1 and mpl_toolkits.axisartist provies the same functionality instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import iris\n",
    "import matplotlib.pyplot as plt\n",
    "import iris.quickplot as qplt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import iris.coord_categorisation\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearsec=60.0*60.0*24.0*365\n",
    "kg2mol_C = 1000.0/12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_avg_with_areacello(cube,cube_areacello):\n",
    "    cube_areacello_tmp = cube_areacello.copy()\n",
    "    no_time_steps = len(cube.coord('time').points)\n",
    "    tmp = cube.copy()\n",
    "    if no_time_steps > 1:\n",
    "        tmp.data = cube.data * np.tile(cube_areacello_tmp.data,[no_time_steps,1,1])\n",
    "    else:\n",
    "        tmp.data = cube.data * cube_areacello_tmp.data\n",
    "    cube_areacello_masked = cube_areacello_tmp.copy()\n",
    "    cube_areacello_masked.data = np.ma.masked_array(cube_areacello_masked.data)\n",
    "    if no_time_steps > 1:\n",
    "        cube_areacello_masked.data.mask = cube[0].data.mask\n",
    "    else:\n",
    "        cube_areacello_masked.data.mask = cube.data.mask\n",
    "    tmp /= np.ma.mean(cube_areacello_masked.data)\n",
    "    return tmp.collapsed(['latitude','longitude'],iris.analysis.MEAN)\n",
    "\n",
    "\n",
    "def area_sum_with_areacello(cube,cube_areacello):\n",
    "    cube_areacello_tmp = cube_areacello.copy()\n",
    "    no_time_steps = len(cube.coord('time').points)\n",
    "    tmp = cube.copy()\n",
    "    if no_time_steps > 1:\n",
    "        tmp.data = cube.data * np.tile(cube_areacello_tmp.data,[no_time_steps,1,1])\n",
    "    else:\n",
    "        tmp.data = cube.data * cube_areacello_tmp.data\n",
    "    cube_areacello_masked = cube_areacello_tmp.copy()\n",
    "    cube_areacello_masked.data = np.ma.masked_array(cube_areacello_masked.data)\n",
    "    if no_time_steps > 1:\n",
    "        cube_areacello_masked.data.mask = cube[0].data.mask\n",
    "    else:\n",
    "        cube_areacello_masked.data.mask = cube.data.mask\n",
    "    return tmp.collapsed(['latitude','longitude'],iris.analysis.SUM)\n",
    "\n",
    "\n",
    "def model_names(directory):\n",
    "\tfiles = glob.glob(directory+'/*.nc')\n",
    "\tmodels_tmp = []\n",
    "\tfor file in files:\n",
    "\t\tstatinfo = os.stat(file)\n",
    "\t\tif statinfo.st_size >= 1:\n",
    "\t\t\tmodels_tmp.append(file.split('/')[-1].split('_')[0])\n",
    "\t\t\tmodels = np.unique(models_tmp)\n",
    "\treturn models\n",
    "\n",
    "def model_names_var(directory,var,grid):\n",
    "\tfiles = glob.glob(directory+'/*'+var+'*'+grid+'*.nc')\n",
    "\tmodels_tmp = []\n",
    "\tfor file in files:\n",
    "\t\tstatinfo = os.stat(file)\n",
    "\t\tif statinfo.st_size >= 1:\n",
    "\t\t\tmodels_tmp.append(file.split('/')[-1].split('_')[0])\n",
    "\t\t\tmodels = np.unique(models_tmp)\n",
    "\treturn models\n",
    "\n",
    "def model_names_var_cmip5(directory,var):\n",
    "\tfiles = glob.glob(directory+'/*'+var+'*.nc')\n",
    "\tmodels_tmp = []\n",
    "\tfor file in files:\n",
    "\t\tstatinfo = os.stat(file)\n",
    "\t\tif statinfo.st_size >= 1:\n",
    "\t\t\tmodels_tmp.append(file.split('/')[-1].split('_')[0])\n",
    "\t\t\tmodels = np.unique(models_tmp)\n",
    "\treturn models\n",
    "\n",
    "def mask_where_zero(cube):\n",
    "    cube.data = np.ma.masked_array(cube.data)\n",
    "    cube.data.fill_value= 9.99e9\n",
    "    cube.data[np.where(cube.data == 0.0)] = 9.99e9\n",
    "    cube.data = np.ma.masked_where(cube.data == 9.99e9,cube.data)\n",
    "    return cube\n",
    "\n",
    "def extract_region(cube,lon_west,lon_east,lat_south,lat_north):\n",
    "    cube_region_tmp = cube.intersection(longitude=(lon_west, lon_east))\n",
    "    cube_region = cube_region_tmp.intersection(latitude=(lat_south, lat_north))\n",
    "    return cube_region\n",
    "\n",
    "\n",
    "def area_sum(cube):\n",
    "#     first_dim = cube.coord(dimensions=1).long_name #latitude\n",
    "#     second_dim = cube.coord(dimensions=2).long_name #longitude\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    area_avged_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.SUM, weights=grid_areas)\n",
    "    return area_avged_cube\n",
    "\n",
    "def area_avg(cube):\n",
    "#     first_dim = cube.coord(dimensions=1).long_name #latitude\n",
    "#     second_dim = cube.coord(dimensions=2).long_name #longitude\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    area_avged_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas)\n",
    "    return area_avged_cube\n",
    "\n",
    "def area_sum2(cube,lon_west,lon_east,lat_south,lat_north):\n",
    "    cube = extract_region(cube,lon_west,lon_east,lat_south,lat_north)\n",
    "#     first_dim = cube.coord(dimensions=1).long_name #latitude\n",
    "#     second_dim = cube.coord(dimensions=2).long_name #longitude\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    area_avged_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.SUM, weights=grid_areas)\n",
    "    return area_avged_cube\n",
    "\n",
    "def area_avg2(cube,lon_west,lon_east,lat_south,lat_north):\n",
    "    cube = extract_region(cube,lon_west,lon_east,lat_south,lat_north)\n",
    "#     first_dim = cube.coord(dimensions=1).long_name #latitude\n",
    "#     second_dim = cube.coord(dimensions=2).long_name #longitude\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    area_avged_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas)\n",
    "    return area_avged_cube\n",
    "\n",
    "def avg_years(cube,start_yr,end_yr):\n",
    "    try:\n",
    "        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    except:\n",
    "        pass\n",
    "    loc = np.where((cube.coord('year').points >= start_yr) & (cube.coord('year').points <= end_yr))\n",
    "    if len(loc[0]) > 0:\n",
    "        return cube[loc].collapsed('time', iris.analysis.MEAN)\n",
    "    else:\n",
    "        cube=cube.collapsed('time', iris.analysis.MEAN)\n",
    "        cube.data[:] = np.nan\n",
    "        return cube\n",
    "\n",
    "                   \n",
    "def return_years(cube):\n",
    "    try:\n",
    "        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    except:\n",
    "        pass\n",
    "    return cube.coord('year').points\n",
    "\n",
    "\n",
    "def year_mean(cube):\n",
    "    try:\n",
    "        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    except:\n",
    "        pass\n",
    "    return cube.aggregated_by('year', iris.analysis.MEAN)\n",
    "\n",
    "def populate_dict(data_dict,directory,models,variable,run,test_value):\n",
    "    lon_west,lon_east,lat_south,lat_north=-80.0,10,0.0,80.0\n",
    "    for model in models:\n",
    "        go_ahead = True\n",
    "        print model\n",
    "        exists = os.path.isfile(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "        if exists:\n",
    "            cube = iris.load_cube(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "#             cube = cube.collapsed(['depth'],iris.analysis.MEAN)\n",
    "            iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "            cube = cube.aggregated_by('year', iris.analysis.MEAN)\n",
    "            test = cube.collapsed(['latitude','longitude','time'],iris.analysis.MEAN)\n",
    "            if test_value:\n",
    "                if test.data > 0.6e-9:\n",
    "                    go_ahead=False\n",
    "            if go_ahead:\n",
    "                first_dim = cube.coord(dimensions=1).long_name #latitude\n",
    "                if first_dim == 'latitude':\n",
    "    #                 cube.coord(dimensions=1).rename('latitude')\n",
    "    #                 cube.coord(dimensions=2).rename('longitude')\n",
    "                    cube = extract_region(cube,lon_west,lon_east,lat_south,lat_north)\n",
    "                    data_dict[run][variable][model] = {}\n",
    "                    data_dict[run][variable][model]['timeseries'] = area_sum(cube)\n",
    "                    data_dict[run][variable][model]['timeseries_avg'] = area_avg(cube)\n",
    "                    data_dict[run][variable][model]['years'] = return_years(cube)\n",
    "                    data_dict[run][variable][model]['first20'] = first_20_avg = avg_years(cube,2006,2026)\n",
    "                    data_dict[run][variable][model]['last20'] = last_20_avg = avg_years(cube,2079,2099)\n",
    "        else:\n",
    "            print directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc does not exist'\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def populate_dict_region(data_dict_region,directory,models,variables,runs,test_value,region_bounds):\n",
    "    regions = list(region_bounds)\n",
    "    #     lon_west,lon_east,lat_south,lat_north = W,E,S,N\n",
    "    for i,region in enumerate(regions):\n",
    "        W,E,S,N = region_bounds[region]['W'],region_bounds[region]['E'],region_bounds[region]['S'],region_bounds[region]['N']\n",
    "        for run in runs:\n",
    "            for variable in variables:\n",
    "                for model in models:\n",
    "                    exists = os.path.isfile(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "                    if exists:\n",
    "                        cube = iris.load_cube(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "                        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "                        cube = cube.aggregated_by('year', iris.analysis.MEAN)\n",
    "                        cube = extract_region(cube,W,E,S,N)\n",
    "                        data_dict_region[region][run][variable][model] = {}\n",
    "                        data_dict_region[region][run][variable][model]['timeseries'] = area_sum(cube)\n",
    "                        data_dict_region[region][run][variable][model]['timeseries_avg'] = area_avg(cube)\n",
    "                        data_dict_region[region][run][variable][model]['years'] = return_years(cube)\n",
    "                        data_dict_region[region][run][variable][model]['first20'] = avg_years(cube,2006,2026)\n",
    "                        data_dict_region[region][run][variable][model]['last20'] = avg_years(cube,2079,2099)\n",
    "                    else:\n",
    "                        print directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc does not exist'\n",
    "    return data_dict_region\n",
    "\n",
    "\n",
    "def populate_dict_region_profile(data_dict_region_profiles,directory,models,variables,runs,test_value,region_bounds):\n",
    "    regions = list(region_bounds)\n",
    "    #     lon_west,lon_east,lat_south,lat_north = W,E,S,N\n",
    "    for i,region in enumerate(regions):\n",
    "        W,E,S,N = region_bounds[region]['W'],region_bounds[region]['E'],region_bounds[region]['S'],region_bounds[region]['N']\n",
    "        data_dict_region_profiles[region]={}\n",
    "        for run in runs:\n",
    "            data_dict_region_profiles[region][run]={}\n",
    "            for variable in variables:\n",
    "                data_dict_region_profiles[region][run][variable]={}\n",
    "                for model in models:\n",
    "                    data_dict_region_profiles[region][run][variable][model]={}\n",
    "                    print run, variable, model\n",
    "                    exists = os.path.isfile(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "                    if exists:\n",
    "                        print 'ok'\n",
    "                        cube = iris.load_cube(directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc')\n",
    "                        iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "                        cube = cube.aggregated_by('year', iris.analysis.MEAN)\n",
    "                        tmp_cube = area_avg(extract_region(cube,W,E,S,N))\n",
    "                        first20_profile = avg_years(tmp_cube,2006,2026)\n",
    "                        last20_profile = avg_years(tmp_cube,2079,2099)\n",
    "                        depth = first20_profile.coord('depth').points\n",
    "                        data_dict_region_profiles[region][run][variable][model]['first20_profile'] = first20_profile.data\n",
    "                        data_dict_region_profiles[region][run][variable][model]['last20_profile'] = last20_profile.data\n",
    "                        data_dict_region_profiles[region][run][variable][model]['depths'] = depth\n",
    "                    else:\n",
    "                        print directory+model+'_'+variable+'_'+run+'_r1i1p1_regridded.nc does not exist'\n",
    "    return data_dict_region_profiles\n",
    "\n",
    "\n",
    "def mask_cube_if_no_mask(cube):\n",
    "    try:\n",
    "        dummy = cube.data.mask\n",
    "    except:\n",
    "        test = np.where(cube.data == np.nan)\n",
    "        if test > 1e3:\n",
    "            tmp = cube.data.copy()\n",
    "            tmp = np.ma.masked_array(tmp)\n",
    "            tmp2 = ma.masked_where(tmp.data == np.nan,tmp)\n",
    "            cube.data = tmp2\n",
    "        test = np.where(cube.data == 0.0)\n",
    "        if test > 1e3:\n",
    "            tmp = cube.data.copy()\n",
    "            tmp = np.ma.masked_array(tmp)\n",
    "            tmp2 = ma.masked_where(tmp.data == 0.0,tmp)\n",
    "            cube.data = tmp2\n",
    "    return cube\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest-data\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def carbchem_revelle(op_swtch,mdi,T_cube,S_cube,TCO2_cube,TALK_cube,Pr=0.0,TB=0.0,Ni=100.0,Tl=1.0e-5):\n",
    "# This function calculates the inorganic carbon chemistry balance\n",
    "# according to the method of Peng et al 1987\n",
    "# The parameters are set in the first few lines\n",
    "\n",
    "#salinity needs to be converted into psu\n",
    "#TCO2 and TALK must be in mol/kg\n",
    "#the ones below here are not needed\n",
    "\n",
    "# This procedure calculates the inorganic carbon chemistry balance\n",
    "# according to the method of Peng et al 1987\n",
    "# The parameters are set in the first few lines\n",
    "#\n",
    "#  ops= 0 ;  output is iteration count\n",
    "#       1 ;            pCO2\n",
    "#       2 ;            pH\n",
    "#       3 ;            [H2CO3]\n",
    "#       4 ;            [HCO3]\n",
    "#       5 ;            [CO3]\n",
    "#       6 ;            satn [co3] : calcite\n",
    "#       7 ;            saturation state: calcite\n",
    "#       8 ;            satn [CO3] : aragonite\n",
    "#       9 ;            saturation state: aragonite\n",
    "#\t10;            Ravelle factor (DIC) calculated from Egleston et al. 2010\n",
    "#\t11;            Alkalinity buffer factor calculated from Egleston et al. 2010\n",
    "\n",
    "    #make sure grids are same size\n",
    "    #make sure rthey years are the same\n",
    "    #extarct the data from the cubes\n",
    "    \n",
    "# from iris import *\n",
    "# from iris.analysis import *\n",
    "# import iris.analysis\n",
    "# from numpy import *\n",
    "# from matplotlib.pyplot import *\n",
    "# from scipy.stats.mstats import *\n",
    "# import iris.plot as iplt\n",
    "# import seawater\n",
    "# import numpy\n",
    "# import iris.quickplot as quickplot\n",
    "# import iris.analysis.stats as istats\n",
    "# temp = iris.load_cube('/home/ph290/tmp/hadgem2es_potential_temperature_historical_regridded.nc').extract(Constraint(depth = 0))\n",
    "# sal = iris.load_cube('/home/ph290/tmp/hadgem2es_salinity_historical_regridded.nc').extract(Constraint(depth = 0))\n",
    "# carb = iris.load_cube('/home/ph290/tmp/hadgem2es_dissolved_inorganic_carbon_historical_regridded.nc').extract(Constraint(depth = 0))\n",
    "# alk = iris.load_cube('/home/ph290/tmp/hadgem2es_total_alkalinity_historical_regridded.nc').extract(Constraint(depth = 0))\n",
    "# import carbchem\n",
    "# co2 = carbchem.carbchem(1,temp.data.fill_value,temp,sal,carb,alk)\n",
    "# T_cube = temp\n",
    "# S_cube = sal\n",
    "# TCO2_cube = carb\n",
    "# TALK_cube = alk  \n",
    "# mdi = temp.data.fill_value\n",
    "\t\n",
    "    t_lat = np.size(T_cube.coord('latitude').points)    \n",
    "    s_lat = np.size(S_cube.coord('latitude').points)\n",
    "    c_lat = np.size(TCO2_cube.coord('latitude').points)\n",
    "    a_lat = np.size(TALK_cube.coord('latitude').points)\n",
    "    lat_test = t_lat == s_lat == c_lat == a_lat\n",
    "\n",
    "    t_lon = np.size(T_cube.coord('longitude').points) \n",
    "    s_lon = np.size(S_cube.coord('longitude').points)\n",
    "    c_lon = np.size(TCO2_cube.coord('longitude').points)\n",
    "    a_lon = np.size(TALK_cube.coord('longitude').points)\n",
    "    lon_test = t_lon == s_lon == c_lon == a_lon\n",
    "\n",
    "    if lat_test and lon_test:\n",
    "\n",
    "        output_cube = T_cube.copy()\n",
    "        T_cube = T_cube\n",
    "        T = T_cube.data.copy()\n",
    "        S = S_cube.data.copy()\n",
    "        TCO2_cube = TCO2_cube/1026.0\n",
    "        # \t\tTCO2 = np.roll(ma.swapaxes(TCO2_cube.data.copy(),0,1),180)\n",
    "        TCO2=TCO2_cube.data.copy()\n",
    "        #NOTE - this is only required here 'cos glodap and WOA are differently ordered - not necessary for other stuff\n",
    "        TALK_cube = TALK_cube/1026.0\n",
    "        # \t\tTALK = np.roll(ma.swapaxes(TALK_cube.data.copy(),0,1),180)\n",
    "        TALK = TALK_cube.data.copy()\n",
    "\n",
    "#         print np.mean(T)\n",
    "#         print np.mean(S)\n",
    "#         print np.mean(TCO2)\n",
    "#         print np.mean(TALK)\n",
    "#         print np.shape(T)\n",
    "#         print np.shape(S)\n",
    "#         print np.shape(TCO2)\n",
    "#         print np.shape(TALK)\n",
    "        msk1=ma.masked_greater_equal(T,mdi-1.0,copy=True)\n",
    "        msk2=ma.masked_greater_equal(S,mdi-1.0,copy=True)\n",
    "        msk3=ma.masked_greater_equal(TCO2,mdi-1.0,copy=True)\n",
    "        msk4=ma.masked_greater_equal(TALK,mdi-1.0,copy=True)\n",
    "\n",
    "        msk=msk1.mask | msk2.mask | msk3.mask | msk4.mask\n",
    "\n",
    "        T[msk]=np.nan\n",
    "        S[msk]=np.nan\n",
    "        TALK[msk]=np.nan\n",
    "        TCO2[msk]=np.nan\n",
    "        # \t\tplt.contourf(T)\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(TCO2)\n",
    "        # \t\tplt.show()\n",
    "\n",
    "        # T = np.array([13.74232016,25.0])\n",
    "        # S = np.array([33.74096661,35.0])\n",
    "        # TCO2 = np.array([0.0019863,2.0e-3])\n",
    "        # TALK = np.array([0.00226763,2.2e-3])\n",
    "        # msk = ma.masked_greater_equal(T,mdi-1.0,copy=True)\n",
    "\n",
    "        #create land-sea mask used by sea_msk.mask\n",
    "        salmin = 1.0\n",
    "        S2=np.copy(S)\n",
    "        S2[np.abs(S) < salmin]=salmin\n",
    "\n",
    "        tol = Tl\n",
    "        mxiter = Ni\n",
    "\n",
    "        op_fld = np.empty(T.shape)\n",
    "        op_fld.fill(np.NAN)\n",
    "\n",
    "        #    TB = np.ones(T.shape)\n",
    "        #    TB = 4.106e-4*S2/35.0\n",
    "        TB = np.empty_like(T)\n",
    "        TB = np.multiply(S2,4.106e-4/35.0, TB)\n",
    "        # this boron is from Peng\n",
    "\n",
    "        #convert to Kelvin\n",
    "        TK=np.copy(T[:])\n",
    "        TK += +273.15\n",
    "\n",
    "        alpha_s = np.ones(T.shape)\n",
    "        alpha_s = np.exp( ( -60.2409 + 9345.17/TK  + 23.3585*np.log(TK/100.0) )  + ( 0.023517 - 0.023656*(TK/100.0) + 0.0047036*np.power((TK/100.0),2.0) )*S )\n",
    "\n",
    "        K1 = np.ones(T.shape)\n",
    "        K1 = np.exp( ( -2307.1266/TK + 2.83655  - 1.5529413*np.log(TK) ) - ( 4.0484/TK + 0.20760841 )*np.sqrt(S) + 0.08468345*S - 0.00654208*np.power(S,1.5) + np.log( 1.0 - 0.001005*S ) )\n",
    "\n",
    "        a = np.array([-25.50,-15.82,-29.48,-25.60,-48.76,-46.0])\n",
    "        b = np.array([0.1271,0.0219,0.2324,0.5304,0.5304])\n",
    "        c = np.array([0.0,0.0,0.0026080,0.0036246,0.0,0.0])\n",
    "        d = np.array([-3.08,1.13,(-2.84e-3)/(1.0e-3),-5.13,-11.76,-11.76])\n",
    "        e = np.array([0.0877,0.1475,0.0,0.0794,0.3692,0.3692])\n",
    "\n",
    "        if keyword.iskeyword(Pr):\n",
    "            instance = 0\n",
    "            pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "            K1 = K1*pf\n",
    "\n",
    "        K2 = np.ones(T.shape)\n",
    "        K2 = np.exp( ( -3351.6106/TK - 9.226508 - 0.2005743*np.log(TK) ) - ( 23.9722/TK + 0.106901773 )*np.power(S,0.5) + 0.1130822*S - 0.00846934*np.power(S,1.5) + np.log( 1.0 - 0.001005*S ) )\n",
    "\n",
    "        if keyword.iskeyword(Pr):\n",
    "            instance = 1\n",
    "            pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "            K2 = K2*pf\n",
    "\n",
    "        KB = np.ones(T.shape)\n",
    "        KB = np.exp( ( -8966.90 - 2890.53*np.power(S,0.5) - 77.942*S + 1.728*np.power(S,1.5)- 0.0996*np.power(S,2.0) )/TK + ( 148.0248 + 137.1942*np.power(S,0.5) + 1.62142*S ) - ( 24.4344 + 25.085*np.power(S,0.5) + 0.2474*S )*np.log(TK) + 0.053105*(np.power(S,0.5))*TK )\n",
    "\n",
    "        if keyword.iskeyword(Pr):\n",
    "            instance = 2\n",
    "            pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "            KB = KB*pf\n",
    "\n",
    "        KW = np.ones(T.shape)\n",
    "        KW = np.exp( ( -13847.26/TK + 148.96502 - 23.6521*np.log(TK) ) + ( 118.67/TK - 5.977 + 1.0495*np.log(TK) )*np.power(S,0.5) - 0.01615*S )\n",
    "\n",
    "        if keyword.iskeyword(Pr):\n",
    "            instance = 3\n",
    "            pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "            KW = KW*pf\n",
    "\n",
    "        if ( op_swtch >= 6 or op_swtch <= 9 ):\n",
    "            ca_conc = np.ones(T.shape)\n",
    "            ca_conc = 0.01028*S2/35.0\n",
    "\n",
    "        if ( op_swtch == 6 or op_swtch == 7 ):\n",
    "            K_SP_C = np.ones(T.shape)\n",
    "            K_SP_C = np.power(10.0,( ( -171.9065 - 0.077993*TK + 2839.319/TK + 71.595*np.log10(TK) ) + ( -0.77712 + 0.0028426*TK + 178.34/TK )*np.power(S,0.5) - 0.07711*S+ 0.0041249*np.power(S,1.5) ))\n",
    "            if keyword.iskeyword(Pr):\n",
    "                instance = 4\n",
    "                pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "                K_SP_C = K_SP_C*pf\n",
    "\n",
    "\n",
    "        if ( op_swtch == 8 or op_swtch == 9 ):\n",
    "            K_SP_A = np.ones(T.shape)\n",
    "            K_SP_A = np.power(10,( ( -171.945 - 0.077993*TK + 2903.293/TK + 71.595*np.log10(TK) ) + ( -0.068393 + 0.0017276*TK + 88.135/TK )*np.power(S,0.5) - 0.10018*S + 0.0059415*np.power(S,1.5) ))\n",
    "            if keyword.iskeyword(Pr):\n",
    "                instance = 5\n",
    "                pf = pressure_fun(a[instance],b[instance],c[instance],d[instance],e[instance],T)\n",
    "                K_SP_A = K_SP_A*pf\n",
    "\n",
    "\n",
    "        # Get first estimate for H+ concentration.\n",
    "\n",
    "        AC, AW, AB, aH, count = carbiter(T, TCO2, TALK, TB, msk, tol, mxiter, K1, K2, KB, KW)\n",
    "\n",
    "        # \t\tplt.contourf(aH)\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(AC)\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(AW)\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(aH)\n",
    "        # \t\tplt.show()\n",
    "\n",
    "        # now we have aH we can calculate...\n",
    "        denom = np.zeros(T.shape)\n",
    "        H2CO3 = np.zeros(T.shape)\n",
    "        HCO3 = np.zeros(T.shape)\n",
    "        CO3 = np.zeros(T.shape)\n",
    "        pH = np.zeros(T.shape)\n",
    "        pCO2 = np.zeros(T.shape)\n",
    "        if ( op_swtch == 6 or op_swtch == 7 ):\n",
    "            sat_CO3_C = np.zeros(T.shape)\n",
    "        if ( op_swtch == 7 ):\n",
    "            sat_stat_C = np.zeros(T.shape)\n",
    "        if ( op_swtch == 8 or op_swtch == 9 ):\n",
    "            sat_CO3_A = np.zeros(T.shape)\n",
    "        if ( op_swtch == 9 ):\n",
    "            sat_stat_A = np.zeros(T.shape)\n",
    "\n",
    "        denom = np.power(aH,2.0) + K1*aH + K1*K2\n",
    "        H2CO3 = TCO2*np.power(aH,2.0)/denom\n",
    "        HCO3 = TCO2*K1*aH/denom\n",
    "        CO3 = TCO2*K1*K2/denom\n",
    "        # \t\tplt.contourf(K1)\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(aH) -no\n",
    "        # \t\tplt.show()\n",
    "        # \t\tplt.contourf(denom) -no\n",
    "        # \t\tplt.show()\n",
    "\n",
    "        pH = -np.log10(aH)\n",
    "        pCO2 = H2CO3/alpha_s\n",
    "\n",
    "        if ( op_swtch == 6 or op_swtch == 7 ):\n",
    "            sat_CO3_C = K_SP_C/ca_conc\n",
    "            if ( op_swtch == 7 ):\n",
    "                sat_stat_C = CO3/sat_CO3_C\n",
    "\n",
    "        if ( op_swtch == 8 or op_swtch == 9 ):\n",
    "            sat_CO3_A = K_SP_A/ca_conc\n",
    "            if ( op_swtch == 9 ):\n",
    "                sat_stat_A = CO3/sat_CO3_A\n",
    "\n",
    "        TALKc=+HCO3+2*(CO3)\n",
    "        var1=HCO3\n",
    "        DIC_buffer=HCO3\n",
    "        ALK_buffer=HCO3\n",
    "\n",
    "        var1=HCO3+4*(CO3)+((aH*AB)/(KB+aH))-AW\n",
    "\n",
    "        DIC_buffer=TCO2-((TALKc*TALKc)/var1)\n",
    "\n",
    "        ALK_buffer=((TALKc*TALKc)-TCO2*var1)/TALKc\n",
    "\n",
    "        output_cube = output_cube*0.0+np.nan\n",
    "        if ( op_swtch == 0 ):\n",
    "            op_fld = np.zeros(T.shape)\n",
    "            op_fld = count\n",
    "        elif ( op_swtch == 1 ):\n",
    "            print np.mean(pCO2)\n",
    "            output_cube.data = pCO2*1.0e6\n",
    "            output_cube.standard_name = 'surface_partial_pressure_of_carbon_dioxide_in_sea_water'\n",
    "            output_cube.long_name = 'CO2 concentration'\n",
    "            output_cube.units = 'uatm'\n",
    "        elif ( op_swtch == 2 ):\n",
    "            output_cube.data = pH\n",
    "            output_cube.standard_name = 'sea_water_ph_reported_on_total_scale'\n",
    "            output_cube.long_name = 'pH'\n",
    "            output_cube.units = '1'\n",
    "        elif ( op_swtch == 3 ):\n",
    "            output_cube.data = H2CO3\n",
    "        elif ( op_swtch == 4 ):\n",
    "            output_cube.data = HCO3\n",
    "        elif ( op_swtch == 5 ):\n",
    "            output_cube.data = CO3\n",
    "        elif ( op_swtch == 6 ):\n",
    "            output_cube.data = sat_CO3_C\n",
    "        elif ( op_swtch == 7 ):\n",
    "            output_cube.data = sat_stat_C\n",
    "        elif ( op_swtch == 8 ):\n",
    "            output_cube.data = sat_CO3_A\n",
    "        elif ( op_swtch == 9 ):\n",
    "            output_cube.data = sat_stat_A\n",
    "        elif ( op_swtch == 10 ):\n",
    "            output_cube.data = TCO2/DIC_buffer\n",
    "        elif ( op_swtch == 11 ):\n",
    "            output_cube.data = ALK_buffer*1000.0\n",
    "\n",
    "        return output_cube\n",
    "\n",
    "\n",
    "'''\n",
    "test-data\n",
    "'''\n",
    "\n",
    "# def main():\n",
    "# mdi=-999.0\n",
    "# sizing=(500,500)\n",
    "# T = np.empty(sizing)\n",
    "# S = np.empty(sizing)\n",
    "# TCO2 = np.empty(sizing)\n",
    "# TALK = np.empty(sizing)\n",
    "# T.fill(10.0)\n",
    "# S.fill(35.0)\n",
    "# TCO2.fill(0.0020)\n",
    "# TALK.fill(0.0022)\n",
    "# T[0,0]=mdi\n",
    "# S[2,3]=mdi\n",
    "# S[0,0]=0.5\n",
    "# TALK[2,3]=mdi\n",
    "# TCO2[2,3]=mdi\n",
    "    \n",
    "#     print carbchem(1,mdi,T,S,TCO2,TALK)\n",
    "\n",
    "# import cProfile\n",
    "# if __name__ == '__main__':\n",
    "#     x=cProfile.run('main()')\n",
    "\n",
    "#main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMIP6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory1 = '/data/BatCaveNAS/ph290/cmip6/regridded/'\n",
    "# directory2 = '/Users/ph290/Downloads/revelle/cmip5/'\n",
    "# directory2 = '/data/BatCaveNAS/ph290/cmip6/processed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'directory2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a46c81c1c14e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_names_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'directory2' is not defined"
     ]
    }
   ],
   "source": [
    "os.listdir(directory2)\n",
    "model_names_var(directory1,variables[0],'gn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['tos','dissic','talk','so','fgco2']\n",
    "\n",
    "gn_models = []\n",
    "gr_models = []\n",
    "try:\n",
    "    gn_models = np.unique(list(model_names_var(directory1,variables[0],'gn')))\n",
    "except:\n",
    "    gr_models = np.unique(list(model_names_var(directory1,variables[0],'gr')))\n",
    "                               \n",
    "models_cmip6 = list(np.unique(list(gn_models) + list(gr_models)))\n",
    "#check to see if all variables available\n",
    "for var in variables[1::]:\n",
    "    gn_models = []\n",
    "    gr_models = []\n",
    "    try:\n",
    "        gn_models = np.unique(list(model_names_var(directory1,var,'gn')))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        gr_models = np.unique(list(model_names_var(directory1,var,'gr')))\n",
    "    except:\n",
    "        pass\n",
    "    models1 = list(np.unique(list(gn_models) + list(gr_models)))\n",
    "    models_cmip6 = list(set(models_cmip6).intersection(models1))\n",
    "#     print var\n",
    "#     print models_cmip6\n",
    "\n",
    "print models_cmip6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def populate_data_dict(directory1,data_dict,model,grid):\n",
    "    lon_west,lon_east,lat_south,lat_north = -180,180,-90,-45\n",
    "    print directory1+model+'_dissic_ssp585_*_'+grid+'_regridded.nc'\n",
    "    file = glob.glob(directory1+model+'_dissic_ssp585_*_'+grid+'_regridded.nc')[0]\n",
    "    ensemble = file.split('_')[3]\n",
    "    start_yr,end_yr = 2015,2035\n",
    "    start_yr2,end_yr2 = 2079,2099\n",
    "\n",
    "    cube_tos = extract_region(mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_tos_ssp585_'+ensemble+'_'+grid+'_regridded.nc'))),lon_west,lon_east,lat_south,lat_north)\n",
    "    cube_sos = extract_region(mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_sos_ssp585_'+ensemble+'_'+grid+'_regridded.nc'))),lon_west,lon_east,lat_south,lat_north)\n",
    "    cube_talk = extract_region(mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_talk_ssp585_'+ensemble+'_'+grid+'_regridded.nc'))),lon_west,lon_east,lat_south,lat_north)\n",
    "    cube_dissic = extract_region(mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_dissic_ssp585_'+ensemble+'_'+grid+'_regridded.nc'))),lon_west,lon_east,lat_south,lat_north)\n",
    "    cube_fgco2 = extract_region(mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_fgco2_ssp585_'+ensemble+'_'+grid+'_regridded.nc'))),lon_west,lon_east,lat_south,lat_north)\n",
    "    \n",
    "    if len(np.shape(cube_talk)) == 4:\n",
    "        name = cube_talk.coord(dimensions=1).long_name\n",
    "        cube_talk = cube_talk.collapsed(name,iris.analysis.MEAN)\n",
    "    if len(np.shape(cube_dissic)) == 4:\n",
    "        name = cube_dissic.coord(dimensions=1).long_name\n",
    "        cube_dissic = cube_dissic.collapsed(name,iris.analysis.MEAN)\n",
    "    if len(np.shape(cube_sos)) == 4:\n",
    "        name = cube_sos.coord(dimensions=1).long_name\n",
    "        cube_dicube_sosssic = cube_sos.collapsed(name,iris.analysis.MEAN)\n",
    "      \n",
    "#     try:\n",
    "#         cube_areacello = cube_areacello.collapsed('time',iris.analysis.MEAN)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "    print cube_talk\n",
    "\n",
    "\n",
    "    cube_tos_start = avg_years(cube_tos,start_yr,end_yr)\n",
    "    cube_sos_start = avg_years(cube_sos,start_yr,end_yr)\n",
    "    cube_talk_start = avg_years(cube_talk,start_yr,end_yr)\n",
    "    cube_dissic_start = avg_years(cube_dissic,start_yr,end_yr)\n",
    "    \n",
    "    if np.max(cube_tos_start.data) < 200:\n",
    "        cube_tos_start += 273.15\n",
    "\n",
    "    revelle_factor = carbchem_revelle(10,9.99e9,cube_tos_start-273.15,cube_sos_start,cube_dissic_start,cube_talk_start)\n",
    "#     revelle_factor = revelle_factor.collapsed('time',iris.analysis.MEAN)\n",
    "\n",
    "    cube_fgco2_area_avg = area_avg(cube_fgco2)\n",
    "    cube_fgco2_area_avg_start = avg_years(cube_fgco2_area_avg,start_yr,end_yr)\n",
    "    cube_fgco2_area_avg_end = avg_years(cube_fgco2_area_avg,start_yr2,end_yr2)\n",
    "    cube_fgco2_area_avg_change = cube_fgco2_area_avg_end - cube_fgco2_area_avg_start\n",
    "\n",
    "    cube_fgco2_area_sum = area_sum(cube_fgco2)\n",
    "    cube_fgco2_area_sum_start = avg_years(cube_fgco2_area_sum,start_yr,end_yr)\n",
    "    cube_fgco2_area_sum_end = avg_years(cube_fgco2_area_sum,start_yr2,end_yr2)\n",
    "    cube_fgco2_area_sum_change = cube_fgco2_area_sum_end - cube_fgco2_area_sum_start\n",
    "    \n",
    "    data_dict[model]['tos_map_start'] = cube_tos_start\n",
    "    data_dict[model]['sos_map_start'] = cube_sos_start\n",
    "    data_dict[model]['dissic_map_start'] = cube_dissic_start\n",
    "    data_dict[model]['talk_map_start'] = cube_talk_start\n",
    "    data_dict[model]['fgco2_map_start'] = avg_years(cube_fgco2,start_yr,end_yr)\n",
    "    data_dict[model]['tos_map_end'] = avg_years(cube_tos,start_yr2,end_yr2)\n",
    "    data_dict[model]['sos_map_end'] = avg_years(cube_sos,start_yr2,end_yr2)\n",
    "    data_dict[model]['dissic_map_end'] = avg_years(cube_dissic,start_yr2,end_yr2)\n",
    "    data_dict[model]['talk_map_end'] = avg_years(cube_talk,start_yr2,end_yr2)\n",
    "    data_dict[model]['fgco2_map_end'] = avg_years(cube_fgco2,start_yr2,end_yr2)\n",
    "    data_dict[model]['revelle_factor_map'] = revelle_factor\n",
    "    data_dict[model]['revelle_factor_mean'] = area_avg_with(revelle_factor,)\n",
    "    data_dict[model]['fgco2_change'] = cube_fgco2_area_avg_change\n",
    "    data_dict[model]['fgco2_change_mean'] = cube_fgco2_area_avg_change\n",
    "    data_dict[model]['fgco2_change_sum'] = cube_fgco2_area_sum_change\n",
    "    data_dict[model]['label'] = 'cmip6'\n",
    "    \n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict={}\n",
    "for model in models_cmip6:\n",
    "    print model\n",
    "    data_dict[model]={}\n",
    "    try:\n",
    "        grid = 'gn'\n",
    "        data_dict = populate_data_dict(directory1,data_dict,model,grid)\n",
    "    except:\n",
    "        grid = 'gr'\n",
    "        data_dict = populate_data_dict(directory1,data_dict,model,grid)\n",
    "# except:\n",
    "#     grid = 'gr'\n",
    "#     data_dict = populate_data_dict(directory1,data_dict,model,grid)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMIP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['dissic','talk','tos','sos','fgco2','areacello']\n",
    "\n",
    "\n",
    "models_cimp5 = list(model_names_var_cmip5(directory2,variables[0]))\n",
    "\n",
    "#check to see if all variables available\n",
    "for var in variables[1::]:\n",
    "    models1 = list(model_names_var_cmip5(directory2,var))\n",
    "    models_cimp5 = list(set(models_cimp5).intersection(models1))\n",
    "\n",
    "print models_cimp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_data_dict_cmip5(directory1,data_dict,model):\n",
    "    run = 'rcp85'\n",
    "#     print directory1+model+'_dissic_+'_Socean.nc'  HadGEM2-CC_dissic_rcp85_r1i1p1_Socean.nc\n",
    "    file = glob.glob(directory1+model+'_dissic_'+run+'_*_Socean.nc')[0]\n",
    "    ensemble = file.split('_')[3]\n",
    "#     start_yr,end_yr = 2015,2035\n",
    "    start_yr,end_yr = 2005,2025\n",
    "\n",
    "\n",
    "    start_yr2,end_yr2 = 2079,2099\n",
    "\n",
    "    cube_tos = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_tos_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "    cube_sos = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_sos_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "    cube_talk = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_talk_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "    cube_dissic = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_dissic_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "    cube_fgco2 = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_fgco2_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "    cube_areacello = iris.load_cube(directory1+model+'_areacello_cmip5_Socean.nc')\n",
    "    if model == 'MPI-ESM-LR':\n",
    "        cube_areacello.data = np.flipud(cube_areacello.data)\n",
    "#     try:\n",
    "#         cube_areacello = cube_areacello.collapsed('time',iris.analysis.MEAN)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "    cube_tos_start = avg_years(cube_tos,start_yr,end_yr)\n",
    "    cube_sos_start = avg_years(cube_sos,start_yr,end_yr)\n",
    "    cube_talk_start = avg_years(cube_talk,start_yr,end_yr)\n",
    "    cube_dissic_start = avg_years(cube_dissic,start_yr,end_yr)\n",
    "\n",
    "    if np.max(cube_tos_start.data) < 200:\n",
    "        cube_tos_start += 273.15\n",
    "\n",
    "    revelle_factor = carbchem_revelle(10,9.99e9,cube_tos_start-273.15,cube_sos_start,cube_dissic_start,cube_talk_start)\n",
    "#     revelle_factor = revelle_factor.collapsed('time',iris.analysis.MEAN)\n",
    "\n",
    "    cube_fgco2_area_avg = area_avg_with_areacello(cube_fgco2,cube_areacello)\n",
    "    cube_fgco2_area_avg_start = avg_years(cube_fgco2_area_avg,start_yr,end_yr)\n",
    "    cube_fgco2_area_avg_end = avg_years(cube_fgco2_area_avg,start_yr2,end_yr2)\n",
    "    cube_fgco2_area_avg_change = cube_fgco2_area_avg_end - cube_fgco2_area_avg_start\n",
    "    \n",
    "    cube_fgco2_area_sum = area_sum_with_areacello(cube_fgco2,cube_areacello)\n",
    "    cube_fgco2_area_sum_start = avg_years(cube_fgco2_area_sum,start_yr,end_yr)\n",
    "    cube_fgco2_area_sum_end = avg_years(cube_fgco2_area_sum,start_yr2,end_yr2)\n",
    "    cube_fgco2_area_sum_change = cube_fgco2_area_sum_end - cube_fgco2_area_sum_start\n",
    "    \n",
    "\n",
    "#     cube_fgco2_global = mask_cube_if_no_mask(year_mean(iris.load_cube(directory1+model+'_fgco2_'+run+'_'+ensemble+'.nc','surface_downward_mass_flux_of_carbon_dioxide_expressed_as_carbon')))\n",
    "#     cube_areacello_global = iris.load_cube(directory1+model+'_areacello_cmip5.nc')\n",
    "#     test = cube_fgco2_global.copy()\n",
    "#     test *= 0.0\n",
    "#     test += 1.0\n",
    "#     ocean_area = area_sum_with_areacello(test,cube_areacello_global)\n",
    "    \n",
    "#     tmp = revelle_factor.copy()\n",
    "#     tmp *= 0.0\n",
    "#     tmp += 1.0\n",
    "#     area_avg_revelle = area_sum_with_areacello(revelle_factor,cube_areacello) / area_sum_with_areacello(tmp,cube_areacello)\n",
    "    area_avg_revelle = area_avg_with_areacello(revelle_factor,cube_areacello)\n",
    "    \n",
    "    data_dict[model]['tos_map_start'] = cube_tos_start\n",
    "    data_dict[model]['sos_map_start'] = cube_sos_start\n",
    "    data_dict[model]['dissic_map_start'] = cube_dissic_start\n",
    "    data_dict[model]['talk_map_start'] = cube_talk_start\n",
    "    data_dict[model]['fgco2_map_start'] = avg_years(cube_fgco2,start_yr,end_yr)\n",
    "    data_dict[model]['tos_map_end'] = avg_years(cube_tos,start_yr2,end_yr2)\n",
    "    data_dict[model]['sos_map_end'] = avg_years(cube_sos,start_yr2,end_yr2)\n",
    "    data_dict[model]['dissic_map_end'] = avg_years(cube_dissic,start_yr2,end_yr2)\n",
    "    data_dict[model]['talk_map_end'] = avg_years(cube_talk,start_yr2,end_yr2)\n",
    "    data_dict[model]['fgco2_map_end'] = avg_years(cube_fgco2,start_yr2,end_yr2)\n",
    "    data_dict[model]['revelle_factor_map'] = revelle_factor\n",
    "    data_dict[model]['revelle_factor_mean'] = area_avg_revelle\n",
    "    data_dict[model]['fgco2_change_mean'] = cube_fgco2_area_avg_change\n",
    "    data_dict[model]['fgco2_change_sum'] = cube_fgco2_area_sum_change\n",
    "    data_dict[model]['areacello'] = cube_areacello\n",
    "#     data_dict[model]['fgco2'] = cube_fgco2\n",
    "    data_dict[model]['label'] = 'cmip5'\n",
    "#     data_dict[model]['ocean_area'] = ocean_area\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_cimp5[0]\n",
    "print directory2+model+'_fgco2_'+'rcp85'+'_'+'r1i1p1'+'_Socean.nc'\n",
    "iris.load_cube(directory2+model+'_fgco2_'+'rcp85'+'_'+'r1i1p1'+'_Socean.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_cimp5:\n",
    "    print model\n",
    "    data_dict[model]={}\n",
    "    data_dict = populate_data_dict_cmip5(directory2,data_dict,model)\n",
    "# except:\n",
    "#     grid = 'gr'\n",
    "#     data_dict = populate_data_dict(directory1,data_dict,model,grid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in models_cimp5:\n",
    "#     print model\n",
    "#     print data_dict[model]['ocean_area'][0].data*1.0e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_cimp5:\n",
    "    print model\n",
    "    qplt.contourf(data_dict[model]['areacello'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'MPI-ESM-LR'\n",
    "run = 'rcp85'\n",
    "ensemble = 'r1i1p1'\n",
    "# cube_fgco2 = mask_cube_if_no_mask(year_mean(iris.load_cube(directory2+model+'_fgco2_'+run+'_'+ensemble+'_Socean.nc')))\n",
    "cube_fgco2 = iris.load_cube(directory2+model+'_fgco2_'+run+'_'+ensemble+'_Socean.nc')\n",
    "print np.shape(cube_fgco2)\n",
    "# print cube_fgco2\n",
    "# print cube_fgco2.coord('latitude')\n",
    "# print cube_fgco2.data[0,:]\n",
    "\n",
    "\n",
    "# cube_fgco2.coord('longutude').points\n",
    "# print data_dict[model]['areacello']\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "gs = gridspec.GridSpec(3,1)\n",
    "ax1 = plt.subplot(gs[0,0],projection=ccrs.PlateCarree())\n",
    "#     plt.contourf(data_dict[model]['revelle_factor_map'].data,21)\n",
    "plt.pcolormesh(cube_fgco2[0].data)\n",
    "ax1 = plt.subplot(gs[1,0])\n",
    "plt.pcolormesh(data_dict[model]['areacello'].data)\n",
    "ax1 = plt.subplot(gs[2,0])\n",
    "\n",
    "plt.pcolormesh(data_dict[model]['areacello'].data * cube_fgco2[0].data)\n",
    "\n",
    "\n",
    "plt.colorbar()\n",
    "plt.title(model)\n",
    "#     plt.gca().coastlines()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = iris.load_cube('/Users/ph290/Downloads/revelle/cmip5/MPI-ESM-LR_areacello_cmip5.nc')\n",
    "tmp2 = iris.load_cube('/Users/ph290/Downloads/revelle/cmip5/MPI-ESM-LR_fgco2_rcp85_r1i1p1.nc')[0]\n",
    "# print tmp \n",
    "qplt.contourf(tmp,100)\n",
    "plt.show()\n",
    "qplt.contourf(tmp2)\n",
    "\n",
    "\n",
    "# plt.gca().coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = models_cmip6+models_cimp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "gs = gridspec.GridSpec(len(models),1)\n",
    "for i,model in enumerate(models):\n",
    "    ax1 = plt.subplot(gs[i:i+1,0],projection=ccrs.PlateCarree())\n",
    "#     plt.contourf(data_dict[model]['revelle_factor_map'].data,21)\n",
    "    plt.contourf(data_dict[model]['fgco2_map_start'].data,21)\n",
    "    plt.colorbar()\n",
    "    plt.title(model)\n",
    "#     plt.gca().coastlines()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model= 'IPSL-CM6A-LR'\n",
    "# #     plt.contourf(data_dict[model]['revelle_factor_map'].data,21)\n",
    "# plt.contourf(data_dict[model]['tos_map_start'].data)\n",
    "# plt.colorbar()\n",
    "# plt.title(model)\n",
    "# #     plt.gca().coastlines()\n",
    "    \n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# model= 'GFDL-CM4'\n",
    "# plt.contourf(data_dict[model]['tos_map_start'].data)\n",
    "# plt.colorbar()\n",
    "# plt.title(model)\n",
    "# #     plt.gca().coastlines()\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHY DO THE CMIP5 RUNS BELOW NOT LOOK LIKE PREVIOUSLY WHERE I REGRIDDED AVERYTHING????\n",
    "e.g. why is teh flux different in MPI-ESM-LR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print models_cimp5\n",
    "for model in models:\n",
    "    y = data_dict[model]['fgco2_change_sum'].data*yearsec*1.0e-12\n",
    "#     y = data_dict[model]['fgco2_change_mean'].data*yearsec*kg2mol_C\n",
    "    x = data_dict[model]['revelle_factor_mean'].data\n",
    "    label = data_dict[model]['label']\n",
    "    if label == 'cmip5':\n",
    "        my_color='r'\n",
    "        my_marker = '*'\n",
    "    if label == 'cmip6':\n",
    "        my_color='b'\n",
    "        my_marker = 'o'\n",
    "    if model == 'MIROC-ES2L':\n",
    "        print 'assuming '+model+' has submitted in units of CO2 rather than C'\n",
    "        y *= (12.0/44.01)\n",
    "    plt.scatter(x,y,label=model,s=100,marker=my_marker)\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.4, 1.00))\n",
    "plt.xlabel('Revelle Factor')\n",
    "# plt.ylabel('air-sea CO$_2$ flux change\\n(mol m$^{-1}$ yr$^{-1}$)')\n",
    "plt.ylabel('air-sea CO$_2$ flux change\\n(PgCyr$^{-1}$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi model mean change in update last 20 years - first 20 yeats \n",
    "greyed is where fewer than 66% of models agree on the sign of change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revelle each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtropical relationship between revelle and change in CO2 flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not Just N. Atl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global air-sea flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subpolar relationship between revelle and change in CO2 flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glodap_dic_file ='/data/NAS-ph290/ph290/observations/GLODAPv2_Mapped_Climatologies/GLODAPv2.2016b_MappedClimatologies/GLODAPv2.2016b.TCO2.nc'\n",
    "# glodap_talk_file ='/data/NAS-ph290/ph290/observations/GLODAPv2_Mapped_Climatologies/GLODAPv2.2016b_MappedClimatologies/GLODAPv2.2016b.TAlk.nc'\n",
    "# glodap_dic = iris.load_cube(glodap_dic_file,'moles of dissolved inorganic carbon per unit mass in seawater')[0]/1026.0\n",
    "# glodap_talk = iris.load_cube(glodap_talk_file,'seawater alkalinity expressed as mole equivalent per unit mass')[0]/1026.0\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=-80.0,10,0.0,80.0\n",
    "# glodap_dic_n_atl = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_n_atl = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=-80.0,10,-45,0\n",
    "# glodap_dic_s_atl = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_s_atl = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=-180.0,180.0,-90.0,-45.0\n",
    "# glodap_dic_so = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_so = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=130,360-100,0,60\n",
    "# glodap_dic_np = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_np = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=130,360-100,-45,0\n",
    "# glodap_dic_sp = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_sp = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n",
    "\n",
    "# lon_west,lon_east,lat_south,lat_north=42,100,-45,20\n",
    "# glodap_dic_io = area_avg2(glodap_dic,lon_west,lon_east,lat_south,lat_north).data\n",
    "# glodap_talk_io = area_avg2(glodap_talk,lon_west,lon_east,lat_south,lat_north).data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
