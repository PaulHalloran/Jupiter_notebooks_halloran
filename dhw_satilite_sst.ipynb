{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking DHW nd DHM scripts using current coraltemp satilite data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Potential issues to think about:\n",
    "* DHM v. DHW and associated different MMM algorithms\n",
    "* William uses nightime data only. CMIP Models are at best deaily mean. A night-time mean shoudl not be too far from teh daily mean, but there will presimably be a slight cold bias. Chris Merchan uses 10am in CCI SST as a bst single time estimate of daily mean.\n",
    "* The climatological period for DHW (1985 - 2012) starts just after an El Nino and ends just before an ENSO, so is rather anomolos because it only contains one ENSO event. A climatology from a free-running model run of the smae length is likley to contain more ENEl Nino events, therefore one could argue is more likely to be biased high, meaning that teh DHW values calculated would be lower than reality\n",
    "* The 1997-1998 El Nino (captured by the DHW MMM climatology period) was anomalously big, so statistically a free-running model run, even with perfect ENSO simulation would likely have a lower MMM than observed, wich would make the DHW values calculate from the model higher than reality\n",
    "   ** Note, the standardising year of 1988.3 is chosen for DHW because it allows them to match a precious climatological period\n",
    "    which would not necessarily be a sensble year to use given model metholology\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ph290/miniconda2/lib/python2.7/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: The mpl_toolkits.axes_grid module was deprecated in version 2.1. Use mpl_toolkits.axes_grid1 and mpl_toolkits.axisartist provies the same functionality instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import iris.quickplot as qplt\n",
    "import netCDF4\n",
    "import datetime\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import glob\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from scipy.stats import t\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_region(cube,lon_west,lon_east,lat_south,lat_north):\n",
    "    cube_region_tmp = cube.intersection(longitude=(lon_west, lon_east))\n",
    "    cube_region = cube_region_tmp.intersection(latitude=(lat_south, lat_north))\n",
    "    return cube_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_avg(cube):\n",
    "    try:\n",
    "        cube.coord('latitude').guess_bounds()\n",
    "        cube.coord('longitude').guess_bounds()\n",
    "    except:\n",
    "        pass\n",
    "    grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "    return cube.collapsed(['longitude','latitude'],iris.analysis.MEAN, weights=grid_areas)\n",
    "\n",
    "\n",
    "def area_max(cube):\n",
    "    return cube.collapsed(['longitude','latitude'],iris.analysis.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linregress_3D(y_array):\n",
    "    # y_array is a 3-D array formatted like (time,lon,lat)\n",
    "    # The purpose of this function is to do linear regression using time series of data over each (lon,lat) grid box with consideration of ignoring np.nan\n",
    "    # Construct x_array indicating time indexes of y_array, namely the independent variable.\n",
    "    x_array=np.empty(y_array.shape)\n",
    "    for i in range(y_array.shape[0]): x_array[i,:,:]=i+1 # This would be fine if time series is not too long. Or we can use i+yr (e.g. 2019).\n",
    "    x_array[np.isnan(y_array)]=np.nan\n",
    "    # Compute the number of non-nan over each (lon,lat) grid box.\n",
    "    n=np.sum(~np.isnan(x_array),axis=0)\n",
    "    # Compute mean and standard deviation of time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    x_mean=np.nanmean(x_array,axis=0)\n",
    "    y_mean=np.nanmean(y_array,axis=0)\n",
    "    x_std=np.nanstd(x_array,axis=0)\n",
    "    y_std=np.nanstd(y_array,axis=0)\n",
    "    # Compute co-variance between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    cov=np.nansum((x_array-x_mean)*(y_array-y_mean),axis=0)/n\n",
    "    # Compute correlation coefficients between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    cor=cov/(x_std*y_std)\n",
    "    # Compute slope between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    slope=cov/(x_std**2)\n",
    "    # Compute intercept between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    intercept=y_mean-x_mean*slope\n",
    "    # Compute tstats, stderr, and p_val between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    tstats=cor*np.sqrt(n-2)/np.sqrt(1-cor**2)\n",
    "    stderr=slope/tstats\n",
    "    p_val=t.sf(tstats,n-2)*2\n",
    "    # Compute r_square and rmse between time series of x_array and y_array over each (lon,lat) grid box.\n",
    "    # r_square also equals to cor**2 in 1-variable lineare regression analysis, which can be used for checking.\n",
    "    r_square=np.nansum((slope*x_array+intercept-y_mean)**2,axis=0)/np.nansum((y_array-y_mean)**2,axis=0)\n",
    "    rmse=np.sqrt(np.nansum((y_array-slope*x_array-intercept)**2,axis=0)/n)\n",
    "    # Do further filteration if needed (e.g. We stipulate at least 3 data records are needed to do regression analysis) and return values\n",
    "    n=n*1.0 # convert n from integer to float to enable later use of np.nan\n",
    "    n[n<3]=np.nan\n",
    "    slope[np.isnan(n)]=np.nan\n",
    "    intercept[np.isnan(n)]=np.nan\n",
    "    p_val[np.isnan(n)]=np.nan\n",
    "    r_square[np.isnan(n)]=np.nan\n",
    "    rmse[np.isnan(n)]=np.nan\n",
    "#     return n,slope,intercept,p_val,r_square,rmse\n",
    "    return slope,intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining DHW, DHM and their MMM climatologies\n",
    "\n",
    "### DHW MMM (Skirving definition):\n",
    "\n",
    "* The MMM is calculated from 1985-2012 inclusive\n",
    "* The data from the above period is averaged from daily to monthly\n",
    "* Each calendar month from the 1985-2012 period (i.e. initially all Jans) are taken. Each grid point has a linear trend fitted to it through time, then the value coresponding to 1988.2857 on that linar trend is assigned to that grid point for that month. This is done for each claendar month. Now you have 12 2d fields, one for each month.\n",
    "* Take the maximum of the 12 values for each pixel. This is the DHW MMM.\n",
    "\n",
    "### DHM MMM (Donner method):\n",
    "\n",
    "* The MMM is calculated from 1985 - 2000 inclusive\n",
    "* The data from the above period is averaged from daily to monthly\n",
    "* The data is NOT detrended\n",
    "* The maximum monthly mean value at each point is taken over that interval.\n",
    "\n",
    "*NOTE The DHM MMM should statistically be considerably higher than then DHW MMM becuase it does not remove climate change and will be weighted towards finding warmest montsh towards the latter part of the climatology period, whereas the DHW MMM attempts to adjust eveything to 1988.3 which falls towards teh start of teh period.*\n",
    "\n",
    "### DHW (Skirving definition):\n",
    "\n",
    "* Working with daily data \n",
    "* Using the DHW MMM defined above, subtract the MMM from each daily timestep over your analysis period\n",
    "* After the subtraction described above, **set all values less than 1 to 0**.\n",
    "* for each point in space and time, look back over an 84 day window (no. days in **3 months**: 7 days * 4 weeks * 3 months), and sum up the values and asign the sum to the current day. i.e. sum up over anomalies a 3 month rolling window.\n",
    "* **Divide by 7 to do from DHdays to DHweeks**\n",
    "\n",
    "### DHM (Donner definition):\n",
    "\n",
    "* Working with monthly data \n",
    "* Using the DHM MMM defined above, subtract the MMM from each daily timestep over your analysis period\n",
    "* After the subtraction described above, **set all values less than 0 to 0**.\n",
    "* for each point in space and time, look back over an **4 month window**  and sum up the values and asign the sum to the current month. i.e. sum up over anomalies a 4 month rolling window.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DHM - 1985-2000\n",
    "\n",
    "# https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/joc.3486\n",
    "# the monthly data set for 1982 â€“ 2006 was first detrended using a linear regression, calculated for each month of the year and grid cell. The data set was detrended and centred on 1988\n",
    "# I don't like this - it becoes ver deending on whether 1988 is a warm or cold year/...\n",
    "\n",
    "def mmm_for_dhm(cube):\n",
    "    years_for_mmm_climatology = [1985,2000]\n",
    "    #####################################################\n",
    "    #Avreage months separately!!!!!!!!!!!\n",
    "#####################################################\n",
    "    cube_years = cube.coord('year').points\n",
    "    #subset the data into the bit you want to use to calculate the MMM climatology and the bit you want to calculate DHW on\n",
    "    clim_cube = cube[np.where((cube_years >= years_for_mmm_climatology[0]) & (cube_years <= years_for_mmm_climatology[1]))]\n",
    "    #collapse the months together, taking the maximum value at each lat-lon grid square\n",
    "    mmm_climatology = clim_cube.collapsed('time',iris.analysis.MAX)\n",
    "    return mmm_climatology\n",
    "\n",
    "\n",
    "\n",
    "def mmm_skirving(cube):\n",
    "    cube = cube.aggregated_by(['year','month'], iris.analysis.MEAN)\n",
    "    print 'calculating NOAA Skirving MMM for month:'\n",
    "#     missing_data_value_greater_than = -32768.0\n",
    "#     missing_data_equals = -32768.0\n",
    "    missing_data_equals = cube.data.fill_value\n",
    "    print 'NOTE THIS SHOULD IDEALLY BE USING AN AVERAGE OF NIGHTIME TEMPERATURES, WHICH IS NOT A BED ESTIMATE FOR DAILY MEAN. A GOOD ALTERNATIEV FOR DAILY MEAN IS 10am (whet chris merchant does)'\n",
    "    years_for_mmm_climatology = [1985,2012]\n",
    "    standardisation_date = 1988.2857\n",
    "    mm_cube = cube[0:12].copy()\n",
    "    mm_cube_data = mm_cube.data.copy()\n",
    "    cube_years = cube.coord('year').points\n",
    "    #subset the data into the bit you want to use to calculate the MMM climatology and the bit you want to calculate DHW on\n",
    "    clim_cube = cube[np.where((cube_years >= years_for_mmm_climatology[0]) & (cube_years <= years_for_mmm_climatology[1]))]\n",
    "    clim_cube_detrended = clim_cube.copy()\n",
    "    clim_cube_detrended_data = clim_cube_detrended.data\n",
    "    print np.shape(clim_cube_detrended)\n",
    "    for i,month in enumerate(np.unique(cube.coord('month_number').points)):\n",
    "        print i+1\n",
    "        loc = np.where(clim_cube.coord('month_number').points == month)\n",
    "        tmp = clim_cube_detrended_data[loc,:,:][0]\n",
    "        tmp[np.where(tmp == missing_data_equals )] = np.nan\n",
    "        slope,intercept = linregress_3D(tmp)\n",
    "        x = standardisation_date - years_for_mmm_climatology[0]\n",
    "        y = (slope * x ) + intercept\n",
    "        mm_cube_data[i,:,:] = y\n",
    "    mm_cube.data = mm_cube_data\n",
    "    mmm_climatology = mm_cube.collapsed('time',iris.analysis.MAX)\n",
    "    return mmm_climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhm(cube,mmm_climatology,years_over_which_to_calculate_dhm):\n",
    "    missing_data_value = cube.data.fill_value\n",
    "    #This hsould be given monthly data\n",
    "    # One DHM == 4DHW This is important\n",
    "#     mmm is straight averg not fdetreneded for DHM\n",
    "    #Look at the donna poapers - look over two papers...\n",
    "    #accumulation window is 4 months rather than 3 months.\n",
    "    cube_years = cube.coord('year').points\n",
    "#     print cube_years\n",
    "#     print years_over_which_to_calculate_dhm[0]\n",
    "#     print years_over_which_to_calculate_dhm[1]\n",
    "    main_cube = cube[np.where((cube_years >= years_over_which_to_calculate_dhm[0]) & (cube_years <= years_over_which_to_calculate_dhm[1]))]\n",
    "    main_cube_data = main_cube.data.copy()\n",
    "    main_cube_data[np.where(main_cube_data == missing_data_value )] = np.nan\n",
    "    main_cube.data = main_cube_data\n",
    "    \n",
    "    #subtract the monthly mean climatology from the rest of the data\n",
    "    main_cube -= mmm_climatology # at this stage this is called a hot spot (which is anything greater than the mmm)\n",
    "\n",
    "    #set all values less than 1 to zero\n",
    "#     main_cube.data[np.where(main_cube.data <= 1.0)] = 0.0\n",
    "    #OR\n",
    "    main_cube_data = main_cube.data.copy()\n",
    "    main_cube_data[np.where(main_cube_data < 0.0)] = 0.0\n",
    "#     main_cube_data[np.where(main_cube_data == missing_data_value )] = np.nan\n",
    "    main_cube.data = main_cube_data\n",
    "    \n",
    "    #make a cube to hold the output data\n",
    "    output_cube = main_cube[3::].copy()\n",
    "    output_cube.data[:] = np.nan\n",
    "    output_cube_data = output_cube.data.copy()\n",
    "\n",
    "        #AVEREG OVER A 4 month  window rather than 3 month when it comes to DHW\n",
    "\n",
    "\n",
    "    #loop through from day 112 to the end of the dataset\n",
    "    for i in range(output_cube.shape[0]):\n",
    "#         print i,' of ',output_cube.shape[0]\n",
    "        tmp_data = main_cube[i:i+4].collapsed('time',iris.analysis.SUM)\n",
    "        output_cube_data[i,:,:] = tmp_data.data\n",
    "\n",
    "    #save the output\n",
    "    output_cube.data = output_cube_data\n",
    "    return output_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhw(cube,mmm_climatology,years_over_which_to_calculate_dhw):\n",
    "    cube_years = cube.coord('year').points\n",
    "    #note this is to be uef with daily data...\n",
    "    main_cube = cube[np.where((cube_years > years_over_which_to_calculate_dhw[0]) & (cube_years < years_over_which_to_calculate_dhw[1]))]\n",
    "    #subtract the monthly mean climatology from the rest of the data\n",
    "    main_cube -= mmm_climatology\n",
    "    #set all values less than 1 to zero\n",
    "    main_cube.data[np.where(main_cube.data < 1.0)] = 0.0 \n",
    "\n",
    "    #make a cube to hold the output data\n",
    "    output_cube = main_cube[83::].copy()\n",
    "    output_cube.data[:] = np.nan\n",
    "    output_cube_data = output_cube.data.copy()\n",
    "\n",
    "    #loop through from day 84 to the end of the dataset\n",
    "    for i in range(output_cube.shape[0]):\n",
    "#         print i,' of ',output_cube.shape[0]\n",
    "        #sum the temperatures in that 84 day window and divide result by 7 to get in DHWeeks rather than DHdays\n",
    "        tmp_data = main_cube[i:i+84].collapsed('time',iris.analysis.SUM)/7.0\n",
    "        output_cube_data[i,:,:] = tmp_data.data\n",
    "\n",
    "    #save the output\n",
    "    output_cube.data = output_cube_data\n",
    "    return output_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in satilite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ph290/miniconda2/lib/python2.7/site-packages/iris/fileformats/cf.py:1143: IrisDeprecation: NetCDF default loading behaviour currently does not expose variables which define reference surfaces for dimensionless vertical coordinates as independent Cubes. This behaviour is deprecated in favour of automatic promotion to Cubes. To switch to the new behaviour, set iris.FUTURE.netcdf_promote to True.\n",
      "  warn_deprecated(msg)\n",
      "/Users/ph290/miniconda2/lib/python2.7/site-packages/iris/fileformats/_pyke_rules/compiled_krb/fc_rules_cf_fc.py:1953: UserWarning: Failed to create 'time' dimension coordinate: The points array must be strictly monotonic.\n",
      "Gracefully creating 'time' auxiliary coordinate instead.\n",
      "  error=e_msg))\n"
     ]
    }
   ],
   "source": [
    "directory = '/Users/ph290/Downloads/dhw/'\n",
    "\n",
    "files = glob.glob(directory+'coraltemp_v1.0_region.nc')\n",
    "cube = iris.load_cube(files,'sea_surface_temperature')\n",
    "\n",
    "try:\n",
    "    iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "    iris.coord_categorisation.add_month(cube, 'time', name='month')\n",
    "    iris.coord_categorisation.add_month_number(cube, 'time', name='month_number')\n",
    "    iris.coord_categorisation.add_day_of_month(cube, 'time', name='day_of_month')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_monthly = cube.aggregated_by(['year','month'], iris.analysis.MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_callback(cube, field,files_tmp):\n",
    "#     # there are some bad attributes in the NetCDF files which make the data incompatible for merging.\n",
    "#     cube.attributes.pop('time_coverage_end')\n",
    "#     cube.attributes.pop('date_issued')\n",
    "#     cube.attributes.pop('time_coverage_start')\n",
    "\n",
    "\n",
    "# directory = '/data/BatCaveNAS/ph290/obs/crw_sst/'\n",
    "# files = glob.glob(directory+'coraltemp_v1.0_*.nc')\n",
    "# cubes = iris.load(files,'sea_surface_temperature',callback=my_callback)\n",
    "# # import irisexperimental\n",
    "# # import iris.experimental.equalise_cubes\n",
    "# # import \n",
    "# # iris.util.unify_time_units(cubes)\n",
    "# # iris.experimental.equalise_cubes.equalise_attributes(cubes)\n",
    "# cube = cubes.concatenate_cube()\n",
    "# # cube = iris.load_cube(files[0],'sea_surface_temperature')\n",
    "# try:\n",
    "#     iris.coord_categorisation.add_year(cube, 'time', name='year')\n",
    "#     iris.coord_categorisation.add_month(cube, 'time', name='month')\n",
    "#     iris.coord_categorisation.add_month_number(cube, 'time', name='month_number')\n",
    "#     iris.coord_categorisation.add_day_of_month(cube, 'time', name='day_of_month')\n",
    "# except:\n",
    "#     pass\n",
    "    \n",
    "# # print cubes[0].coord('time')\n",
    "# print cube.coord('day_of_month').points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate DHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_west = 142.0\n",
    "lon_east = 157.0\n",
    "lat_south = -30.0\n",
    "lat_north = -10.0\n",
    "\n",
    "years_over_which_to_calculate_dhw = [2012,2019]\n",
    "# cube_region = extract_region(cube,lon_west,lon_east,lat_south,lat_north)\n",
    "cube_region = extract_region(cube,lon_west,lon_east,lat_south,lat_north)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating NOAA Skirving MMM for month:\n",
      "NOTE THIS SHOULD IDEALLY BE USING AN AVERAGE OF NIGHTIME TEMPERATURES, WHICH IS NOT A BED ESTIMATE FOR DAILY MEAN. A GOOD ALTERNATIEV FOR DAILY MEAN IS 10am (whet chris merchant does)\n",
      "(336, 400, 300)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ph290/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:42: MaskedArrayFutureWarning: setting an item on a masked array which has a shared mask will not copy the mask and also change the original mask array in the future.\n",
      "Check the NumPy 1.11 release notes for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "mmm_climatology_dhw = mmm_skirving(cube_region)\n",
    "dhw_cube_gbr = dhw(cube_region,mmm_climatology_dhw,years_over_which_to_calculate_dhw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_region_monthly = cube_region.aggregated_by(['month','year'], iris.analysis.MEAN)\n",
    "mmm_climatology_dhm = mmm_for_dhm(cube_region_monthly)\n",
    "dhm_cube_gbr_monthly = dhm(cube_region_monthly,mmm_climatology_dhm,years_over_which_to_calculate_dhw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'fill_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f304a0d9b1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdhw_cube_gbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_where\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhw_cube_gbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdhw_cube_gbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdhw_cube_gbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'fill_value'"
     ]
    }
   ],
   "source": [
    "def asb(cube,threshold):\n",
    "    dhvalue_cube_gbr_tmp = cube.copy()\n",
    "    dhvalue_cube_gbr_tmp_data = dhvalue_cube_gbr_tmp.data\n",
    "    dhvalue_cube_gbr_tmp_data[np.where(dhvalue_cube_gbr_tmp_data <= threshold)] = 0.0\n",
    "    dhvalue_cube_gbr_tmp_data[np.where(dhvalue_cube_gbr_tmp_data > threshold)] = 1.0\n",
    "    dhvalue_cube_gbr_tmp.data = dhvalue_cube_gbr_tmp_data\n",
    "    dhvalue_cube_gbr_asb = dhvalue_cube_gbr_tmp.copy()\n",
    "    dhvalue_cube_gbr_asb = dhvalue_cube_gbr_tmp.aggregated_by(['year'], iris.analysis.SUM)\n",
    "    dhvalue_cube_gbr_asb_tmp = dhvalue_cube_gbr_asb.data\n",
    "    dhvalue_cube_gbr_asb_tmp[np.where(dhvalue_cube_gbr_asb_tmp > 1.0)] = 1.0\n",
    "    dhvalue_cube_gbr_asb.data = dhvalue_cube_gbr_asb_tmp\n",
    "    return dhvalue_cube_gbr_asb\n",
    "\n",
    "\n",
    "dhw_cube_gbr.data = np.ma.masked_where(dhw_cube_gbr.data == dhw_cube_gbr.data.fill_value,dhw_cube_gbr.data)\n",
    "\n",
    "\n",
    "# dhm_cube_gbr = extract_region(dhm_cube_gbr,lon_west,lon_east,lat_south,lat_north)\n",
    "dhw_cube_gbr_asb = asb(dhw_cube_gbr,8.0)\n",
    "dhw_cube_gbr_asb_area_avg = area_avg(dhw_cube_gbr_asb)\n",
    "dhw_cube_gbr.data[np.where(dhw_cube_gbr.data == 0.0)] = np.nan\n",
    "dhw_avg = area_avg(dhw_cube_gbr.aggregated_by(['year'], iris.analysis.MEAN))\n",
    "\n",
    "\n",
    "dhm_cube_gbr_asb_monthly = asb(dhm_cube_gbr_monthly,2.0)\n",
    "dhm_cube_gbr_asb_area_avg_monthly = area_avg(dhm_cube_gbr_asb_monthly)\n",
    "dhm_avg_monthly = area_avg(dhm_cube_gbr_monthly.aggregated_by(['year'], iris.analysis.MEAN))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following compares DHW and DHMs where the daily satilite data has been averaged to monthly prior to any DHM analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum **DHW** in each grid box over 2012-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "dhw_cube_gbr_max = dhw_cube_gbr.collapsed('time',iris.analysis.MAX)\n",
    "qplt.pcolormesh(dhw_cube_gbr_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhm_cube_gbr_max = dhm_cube_gbr_monthly.collapsed('time',iris.analysis.MAX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum **DHM** in each grid box over 2012-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "qplt.pcolormesh(dhm_cube_gbr_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time mean **DHW** in each grid box over 2012-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "dhw_cube_gbr_mean = dhw_cube_gbr.collapsed('time',iris.analysis.MEAN)\n",
    "qplt.pcolormesh(dhw_cube_gbr_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time mean **DHM** in each grid box over 2012-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "dhm_cube_gbr_mean = dhm_cube_gbr_monthly.collapsed('time',iris.analysis.MEAN)\n",
    "qplt.pcolormesh(dhm_cube_gbr_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhw_avg = area_avg(dhw_cube_gbr.aggregated_by(['year'], iris.analysis.MEAN))\n",
    "dhm_avg = area_avg(dhm_cube_gbr_monthly.aggregated_by(['year'], iris.analysis.MEAN))\n",
    "\n",
    "dhw_max = area_max(dhw_cube_gbr.aggregated_by(['year'], iris.analysis.MAX))\n",
    "dhm_max = area_max(dhm_cube_gbr_monthly.aggregated_by(['year'], iris.analysis.MAX))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average DHW and DHM in time and space in each grid box over 2012-2019, using region in above plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(dhw_avg.coord('year').points,dhw_avg.data,s=150,label='DHW')\n",
    "plt.scatter(dhm_avg.coord('year').points,dhm_avg.data,s=150,label='DHM')\n",
    "plt.ylabel('avg. regional DHW/DHM')\n",
    "plt.legend()\n",
    "plt.xlabel('year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum DHW and DHM in time and space in each grid box over 2012-2019, using region in above plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(dhw_avg.coord('year').points,dhw_max.data,s=150,label='DHW')\n",
    "plt.scatter(dhm_avg.coord('year').points,dhm_max.data,s=150,label='DHM')\n",
    "plt.ylabel('maximum annual regional DHW/DHM')\n",
    "plt.legend()\n",
    "plt.xlabel('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
